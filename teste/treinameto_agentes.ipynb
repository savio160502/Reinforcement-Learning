{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Definindo os agentes Codificador e Revisor"
      ],
      "metadata": {
        "id": "Y8DmdsopZQ9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "class LLMAgent:\n",
        "    def __init__(self, model_name=\"gpt2\", agent_type=\"codificador\"):\n",
        "        \"\"\"\n",
        "        Inicializa o agente LLM com o modelo GPT-2.\n",
        "\n",
        "        Parameters:\n",
        "        model_name (str): Nome do modelo a ser utilizado (GPT-2 neste exemplo).\n",
        "        agent_type (str): O tipo do agente (\"codificador\" ou \"revisor\").\n",
        "        \"\"\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        self.agent_type = agent_type\n",
        "        self.history = []  # Histórico de interações entre os agentes\n",
        "\n",
        "        # Definir o pad_token_id explicitamente\n",
        "        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
        "\n",
        "    def generate_code(self, prompt):\n",
        "        \"\"\"\n",
        "        Gera um código ou feedback baseado no prompt fornecido.\n",
        "\n",
        "        Parameters:\n",
        "        prompt (str): O enunciado do problema ou o código a ser revisado.\n",
        "\n",
        "        Returns:\n",
        "        str: O código gerado pelo agente.\n",
        "        \"\"\"\n",
        "        # Tokenizar o prompt com attention_mask\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\",  # Retorna tensores do PyTorch\n",
        "            padding=True,  # Adiciona padding automaticamente\n",
        "            truncation=True,  # Corta se o prompt for muito longo\n",
        "            max_length=512  # Limita o comprimento dos tokens se necessário\n",
        "        )\n",
        "\n",
        "        input_ids = inputs[\"input_ids\"]\n",
        "        attention_mask = inputs[\"attention_mask\"]\n",
        "\n",
        "        # Geração com pad_token_id e attention_mask definidos\n",
        "        outputs = self.model.generate(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=100,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=self.tokenizer.pad_token_id  # Evita o warning do pad_token_id\n",
        "        )\n",
        "\n",
        "        #outputs = self.model.generate(inputs, max_length=150, num_return_sequences=1)\n",
        "        #outputs = self.model.generate(inputs, max_new_tokens=100, num_return_sequences=1)\n",
        "\n",
        "        result = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        self.history.append(result)  # Armazena a interação no histórico\n",
        "        return result\n",
        "\n",
        "    def review_code(self, code):\n",
        "        \"\"\"\n",
        "        Revê o código gerado, sugerindo melhorias.\n",
        "\n",
        "        Parameters:\n",
        "        code (str): O código gerado pelo agente codificador.\n",
        "\n",
        "        Returns:\n",
        "        str: O feedback ou revisão do código.\n",
        "        \"\"\"\n",
        "        review_prompt = f\"Reveja o seguinte código e sugira melhorias:\\n\\n{code}\"\n",
        "        return self.generate_code(review_prompt)\n",
        "\n",
        "# Exemplo de uso:\n",
        "# codificador = LLMAgent(agent_type=\"codificador\")\n",
        "# revisor = LLMAgent(agent_type=\"revisor\")\n"
      ],
      "metadata": {
        "id": "BYOtb-TiZcJY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definindo o ambiente para os agentes interagirem"
      ],
      "metadata": {
        "id": "ccsOxfvZZkcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilizaremos a biblioteca Gym para definir um ambiente onde os agentes possam interagir.\n",
        "#  A função do ambiente é atribuir recompensas com base na qualidade do código e do feedback.\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "\n",
        "class DataAnalysisEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Um ambiente personalizado para treinar agentes de RL que resolvem problemas de análise de dados.\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, expected_output, codificador, revisor):\n",
        "        super(DataAnalysisEnv, self).__init__()\n",
        "\n",
        "        # Armazena os agentes LLM\n",
        "        self.codificador = codificador\n",
        "        self.revisor = revisor\n",
        "\n",
        "        # Definindo o espaço de ação e observação\n",
        "        self.action_space = spaces.Discrete(3)  # Exemplo: (0 = continuar, 1 = revisar, 2 = finalizar)\n",
        "        self.observation_space = spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32)\n",
        "\n",
        "        # Dados e solução esperada\n",
        "        self.dataset = dataset\n",
        "        self.expected_output = expected_output\n",
        "        self.current_state = 0  # Representa o progresso do agente (inicializado como 0)\n",
        "        self.max_steps = 10  # Número máximo de tentativas\n",
        "\n",
        "        # Internamente, mantemos um histórico de interações\n",
        "        self.steps_taken = 0\n",
        "        self.done = False\n",
        "        self.current_code = \"\"  # Código gerado pelos agentes\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reseta o ambiente para um novo episódio.\n",
        "        \"\"\"\n",
        "        self.current_state = 0\n",
        "        self.steps_taken = 0\n",
        "        self.done = False\n",
        "        self.current_code = \"\"  # Limpa o código gerado\n",
        "        return np.array([self.current_state], dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Executa uma ação no ambiente (o código gerado ou revisado pelos agentes).\n",
        "\n",
        "        Parameters:\n",
        "        action (int): A ação que o agente deseja executar.\n",
        "\n",
        "        Returns:\n",
        "        Tuple: A nova observação, recompensa, flag de término e informações adicionais.\n",
        "        \"\"\"\n",
        "        self.steps_taken += 1\n",
        "        reward = 0\n",
        "\n",
        "        if action == 0:  # Continuar gerando código\n",
        "            # Codificador gera mais código\n",
        "            new_code = self.codificador.generate_code(f\"Dados: {self.dataset}\\n\")\n",
        "            self.current_code += new_code\n",
        "            reward = -1  # Penalidade por não concluir a tarefa ainda\n",
        "\n",
        "        elif action == 1:  # Revisar código\n",
        "            # Revisor analisa o código e sugere melhorias\n",
        "            feedback = self.revisor.review_code(self.current_code)\n",
        "            reward = 1  # Recompensa por sugerir melhorias (simulação)\n",
        "\n",
        "        elif action == 2:  # Finalizar o processo\n",
        "            # Verifica se o código está correto\n",
        "            if self._evaluate_solution():\n",
        "                reward = 10  # Recompensa por gerar a solução correta\n",
        "                self.done = True\n",
        "            else:\n",
        "                reward = -10  # Penalidade por finalizar incorretamente\n",
        "                self.done = True\n",
        "\n",
        "        # Atualiza o estado do ambiente\n",
        "        self.current_state = np.random.rand()\n",
        "        if self.steps_taken >= self.max_steps:\n",
        "            self.done = True\n",
        "\n",
        "        return np.array([self.current_state], dtype=np.float32), reward, self.done, {}\n",
        "\n",
        "    def _evaluate_solution(self):\n",
        "        \"\"\"\n",
        "        Função de avaliação que compara o código gerado com a saída esperada.\n",
        "        \"\"\"\n",
        "        # Simulação de avaliação. Aqui você pode implementar uma função mais complexa\n",
        "        # para avaliar se o código gerado está correto.\n",
        "        return \"visualização de dados\" in self.current_code\n",
        "\n",
        "\n",
        "# Codificador e Revisor no Ambiente: O ambiente recebe o codificador e o revisor como parâmetros e usa suas funções para gerar e revisar o código em cada passo.\n",
        "\n",
        "# Ação do Codificador: Quando o agente decide continuar gerando código (action == 0), a função generate_code do Codificador é chamada e o código é atualizado no ambiente.\n",
        "\n",
        "# Ação do Revisor: Quando o agente decide revisar o código (action == 1), o Revisor revisa o código gerado e sugere melhorias.\n",
        "\n",
        "# Finalizar: Se o agente decide finalizar o processo (action == 2), a função de avaliação compara o código atual com a solução esperada e atribui uma recompensa dependendo do resultado."
      ],
      "metadata": {
        "id": "UNUWfoO5Zv6E"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Treinamento dos agentes"
      ],
      "metadata": {
        "id": "72qqUJcDZ3Rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Usaremos a biblioteca stable-baselines3 para realizar o treinamento dos agentes.\n",
        "# Usaremos o algoritmo PPO (Proximal Policy Optimization), que é adequado para esse tipo de problema\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "import torch\n",
        "\n",
        "# Verifica se a GPU está disponível e escolhe o dispositivo correto\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def train_agents():\n",
        "    \"\"\"\n",
        "    Treina os agentes codificador e revisor no ambiente de análise de dados.\n",
        "    \"\"\"\n",
        "    # Instanciando os agentes LLM\n",
        "    codificador = LLMAgent(agent_type=\"codificador\")\n",
        "    revisor = LLMAgent(agent_type=\"revisor\")\n",
        "\n",
        "    # Instanciando o ambiente de RL\n",
        "    dataset = \"Walmart.csv\"\n",
        "    #expected_output = \"Solução correta de visualização de dados\"\n",
        "    expected_output = {\n",
        "    \"type\": \"visualization\",\n",
        "    \"description\": \"Gráficos de linha mostrando as vendas ao longo do tempo, separados por categorias de produtos, com uma linha de tendência e intervalos de confiança.\",\n",
        "    \"components\": [\n",
        "        {\"chart_type\": \"line\", \"title\": \"Vendas ao Longo do Tempo\"},\n",
        "        {\"chart_type\": \"bar\", \"title\": \"Vendas por Categoria\"},\n",
        "        {\"chart_type\": \"heatmap\", \"title\": \"Análise de Sazonalidade\"}\n",
        "    ]\n",
        "}\n",
        "    env = DataAnalysisEnv(dataset, expected_output, codificador, revisor)\n",
        "\n",
        "    # Configurando o modelo de aprendizado por reforço PPO para usar GPU, se disponível\n",
        "    model = PPO(\"MlpPolicy\", env, verbose=1, device=device)\n",
        "\n",
        "    # Treinando o modelo por 10.000 passos\n",
        "    model.learn(total_timesteps=10000)\n",
        "\n",
        "    # Salvando o modelo treinado\n",
        "    model.save(\"ppo_agents\")\n",
        "\n",
        "    # Carregar o modelo para uso posterior\n",
        "    # model = PPO.load(\"ppo_agents\")\n",
        "\n",
        "# teste do resultado final\n",
        "def test_trained_model():\n",
        "    \"\"\"\n",
        "    Testa o modelo treinado em um novo episódio.\n",
        "    \"\"\"\n",
        "    # Carrega o modelo treinado\n",
        "    model = PPO.load(\"ppo_agents\")\n",
        "\n",
        "    # Instanciando os agentes\n",
        "    codificador = LLMAgent(agent_type=\"codificador\")\n",
        "    revisor = LLMAgent(agent_type=\"revisor\")\n",
        "\n",
        "    # Instanciando o ambiente\n",
        "    dataset = \"sales_data.csv\"\n",
        "    expected_output = \"Solução correta de visualização de dados\"\n",
        "    env = DataAnalysisEnv(dataset, expected_output, codificador, revisor)\n",
        "\n",
        "    obs = env.reset()\n",
        "\n",
        "    for _ in range(10):\n",
        "        action, _states = model.predict(obs)\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        print(f\"Ação tomada: {action}, Recompensa: {reward}\")\n",
        "\n",
        "        if done:\n",
        "            print(\"Episódio finalizado.\")\n",
        "            break\n",
        "\n",
        "import time\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_agents()\n",
        "\n",
        "    # Teste o modelo treinado\n",
        "    #test_trained_model()"
      ],
      "metadata": {
        "id": "Sxx7DUq8Z8Y9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}