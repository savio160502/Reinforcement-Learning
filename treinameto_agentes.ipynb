{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install pylint stable-baselines3 sentence-transformers mypy ruff bandit shimmy gymnasium"
      ],
      "metadata": {
        "id": "p14Z1ipAWIA1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.logger import configure\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import subprocess\n",
        "import logging\n",
        "import torch\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "h4nP94_3lq0f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar o uso da GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# device = \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "poZjNBoxNo9l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e20dc2da-918d-4217-de9f-3bb2073be6d8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definindo o ambiente"
      ],
      "metadata": {
        "id": "RWdxnIX-l4Zs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataAnalysisEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(DataAnalysisEnv, self).__init__()\n",
        "\n",
        "        # Definindo o espaço de ações e observações para o ambiente\n",
        "        self.action_space = gym.spaces.Discrete(10)  # Ex.: 5 ações para codificador e 5 para revisor\n",
        "        self.observation_space = gym.spaces.Box(low=-1, high=1, shape=(512,), dtype=np.float32)\n",
        "\n",
        "        # Modelos LLM para Codificador e Revisor\n",
        "        self.codificador_llm = pipeline(\"text-generation\", model=\"Salesforce/codegen-350M-mono\", device = 0 if torch.cuda.is_available() else -1)\n",
        "        self.revisor_llm = pipeline(\"text-generation\", model=\"Salesforce/codegen-350M-mono\", device = 0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "        # Variáveis de estado e controle de recompensas\n",
        "        self.state = None\n",
        "        self.done = False\n",
        "        self.rewards = {\"codificador\": 0, \"revisor\": 0}\n",
        "        self.total_score = 0\n",
        "        self.threshold_score = 135\n",
        "\n",
        "        # Prompt inicial para o agente codificador\n",
        "        self.prompt_codificador = \"\"\"\n",
        "        Agente Codificador:\n",
        "        Você é um programador no ramo da ciência de dados.\n",
        "        Seu trabalho é escrever um script para analisar os dados de vendas fornecidos.\n",
        "        O script deve tratar os dados e fazer uma análise temporal das vendas do arquivo \"/content/Walmart.csv\".\n",
        "        \"\"\"\n",
        "\n",
        "        # Prompt inicial para o agente revisor\n",
        "        self.prompt_revisor = \"\"\"\n",
        "        Agente Revisor:\n",
        "        Você é um revisor de código no ramo da ciência de dados.\n",
        "        Seu trabalho é revisar o script de limpeza de dados e análise de dados fornecido pelo programador.\n",
        "        Você deve identificar erros e sugerir melhorias para garantir que o script seja eficiente e correto.\n",
        "        \"\"\"\n",
        "\n",
        "        # Avaliador de relatórios e log do episódio\n",
        "        self.report_evaluator = ReportEvaluator()\n",
        "        self.current_report = \"\"\n",
        "        self.logger = logging.getLogger(\"DataAnalysisEnv\")\n",
        "        self.logger.setLevel(logging.INFO)\n",
        "        self.episode_log = []\n",
        "        self.current_report_score = 0\n",
        "        self.previous_report_score = 0\n",
        "\n",
        "        self.analyzer = CodeAnalyzer() # analizar código\n",
        "\n",
        "    def reset(self):\n",
        "        # Redefine o ambiente e variáveis para um novo episódio\n",
        "        self.state = self._generate_initial_prompt()\n",
        "        self.done = False\n",
        "        self.total_score = 0\n",
        "        self.rewards = {\"codificador\": 0, \"revisor\": 0}\n",
        "        self.episode_log = []\n",
        "\n",
        "        # Reset de relatório e pontuação\n",
        "        self.current_report = \"\"\n",
        "        self.previous_report_score = 0\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        # Ações do Codificador e Revisor\n",
        "        codificador_action = action // 5\n",
        "        revisor_action = action % 5\n",
        "\n",
        "        # Armazena as ações no log do episódio para análise futura\n",
        "        self.episode_log.append({\n",
        "            \"codificador_action\": codificador_action,\n",
        "            \"revisor_action\": revisor_action,\n",
        "            \"codificador_prompt\": self.prompt_codificador,\n",
        "            \"revisor_prompt\": self.prompt_revisor,\n",
        "        })\n",
        "\n",
        "        # Executa ações dos agentes\n",
        "        code = self._execute_codificador_action(codificador_action)\n",
        "        review_result = self._execute_revisor_action(revisor_action)\n",
        "\n",
        "        # Atualiza recompensas e verifica o término do episódio\n",
        "        self._update_rewards(code)\n",
        "        self.done = self.total_score >= self.threshold_score\n",
        "\n",
        "        # Retorna a soma das recompensas como um float\n",
        "        total_reward = float(self.rewards[\"codificador\"] + self.rewards[\"revisor\"])\n",
        "\n",
        "        # Log detalhado para cada passo, mostrando ações, códigos e recompensas\n",
        "        print(f\"Passo: Codificador ação: {codificador_action}, Revisor ação: {revisor_action}\")\n",
        "        print(f\"Código gerado pelo Codificador:\\n{code}\")\n",
        "        print(f\"Revisão feita pelo Revisor:\\n{review_result}\")\n",
        "        print(f\"Recompensas - Codificador: {self.rewards['codificador']}, Revisor: {self.rewards['revisor']}\")\n",
        "        print(f\"Recompensa total: {total_reward}\\n\")\n",
        "\n",
        "        return self.state, total_reward, self.done, {}\n",
        "\n",
        "    def _execute_codificador_action(self, action):\n",
        "        # Mapeamento das ações do codificador usando a LLM\n",
        "        codificador_actions = [\n",
        "            \"Escreva um código de carregamento de dados.\",\n",
        "            \"Faça uma limpeza nos dados.\",\n",
        "            \"Realize uma análise temporal.\",\n",
        "            \"Visualize os resultados.\",\n",
        "            \"Interprete as conclusões.\"\n",
        "        ]\n",
        "        codificador_prompt = self.prompt_codificador + \"\\n\" + codificador_actions[action]\n",
        "        generated_code = self.codificador_llm(codificador_prompt, max_new_tokens=100)[0][\"generated_text\"]\n",
        "        self.current_report += generated_code\n",
        "\n",
        "        return generated_code\n",
        "\n",
        "    def _execute_revisor_action(self, action):\n",
        "        # Mapeamento das ações do revisor usando a LLM\n",
        "        revisor_actions = [\n",
        "            \"Verifique por erros no código.\",\n",
        "            \"Execute o código para ver os resultados.\",\n",
        "            \"Sugira melhorias de performance.\",\n",
        "            \"Aprove o código se estiver correto.\",\n",
        "            \"Melhore o relatório analítico.\"\n",
        "        ]\n",
        "        revisor_prompt = self.prompt_revisor + \"\\n\" + revisor_actions[action]\n",
        "        review_feedback = self.revisor_llm(revisor_prompt, max_new_tokens=100)[0][\"generated_text\"]\n",
        "\n",
        "        self.current_report += \"\\n\" + review_feedback\n",
        "        return review_feedback\n",
        "\n",
        "    def _update_rewards(self,code):\n",
        "        # Calcula as recompensas para Codificador e Revisor\n",
        "        self.rewards[\"codificador\"] = self._calculate_codificador_reward(code)\n",
        "        self.rewards[\"revisor\"] = self._calculate_revisor_reward()\n",
        "        self.total_score += self.rewards[\"codificador\"] + self.rewards[\"revisor\"]\n",
        "\n",
        "    def _calculate_codificador_reward(self,code):\n",
        "        # Recompensa baseada na análise de código estático\n",
        "        errors_count, analysis_details = self.analyzer.analyze_code(code)\n",
        "\n",
        "        if errors_count > 0:\n",
        "            code_analysis_reward = -2 * errors_count  # Penalidade para cada erro\n",
        "        else:\n",
        "            code_analysis_reward = 10  # Recompensa para código sem erros\n",
        "\n",
        "        return code_analysis_reward\n",
        "\n",
        "    def _calculate_revisor_reward(self):\n",
        "        \"\"\"\n",
        "        Calcula a recompensa do revisor com base na melhoria na pontuação do relatório.\n",
        "        \"\"\"\n",
        "        # Gera um dicionário com as seções e textos do relatório atual\n",
        "        current_report_sections = {\n",
        "            \"Descrição do Problema\": self.current_report[:100],  # Exemplo de split\n",
        "            \"Descrição dos Dados\": self.current_report[100:200],\n",
        "            \"Metodologia\": self.current_report[200:300],\n",
        "            \"Resultados\": self.current_report[300:400],\n",
        "            \"Conclusão\": self.current_report[400:]\n",
        "        }\n",
        "\n",
        "        # Avalia o relatório atual e calcula a melhoria\n",
        "        current_report_score = self.report_evaluator.evaluate_report(current_report_sections)\n",
        "        self.total_score = max(0, current_report_score - self.previous_report_score)\n",
        "        self.previous_report_score = current_report_score  # Atualiza a pontuação anterior\n",
        "\n",
        "        # Normaliza a melhoria para um valor escalado de recompensa\n",
        "        if self.total_score >= self.threshold_score:\n",
        "          self.done = True\n",
        "\n",
        "        normalized_improvement = self.total_score / 150\n",
        "\n",
        "        return normalized_improvement * 10  # Multiplica para ajustar ao sistema de recompensas\n",
        "\n",
        "    def _generate_initial_prompt(self):\n",
        "        # Gera o estado inicial do prompt\n",
        "        return np.zeros(self.observation_space.shape)\n",
        "\n",
        "    def log_episode(self, episode_number):\n",
        "        # Gera um resumo detalhado do episódio\n",
        "        episode_summary = {\n",
        "            \"episode_number\": episode_number,\n",
        "            \"actions\": self.episode_log,\n",
        "            \"total_score\": self.total_score,\n",
        "            \"final_rewards\": self.rewards,\n",
        "        }\n",
        "        self.logger.info(f\"Resumo do Episódio {episode_number}: {episode_summary}\")\n"
      ],
      "metadata": {
        "id": "hIM6CbXXWBrT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análise do relatório e do código gerado"
      ],
      "metadata": {
        "id": "4jjPChfumCzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReportEvaluator:\n",
        "    def __init__(self):\n",
        "        self.model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
        "        self.section_prompts = {\n",
        "            \"Descrição do Problema\": [\"Clareza\", \"Acurácia\"],\n",
        "            \"Descrição dos Dados\": [\"Completude\", \"Análise de Qualidade de Dados\", \"Visualização\"],\n",
        "            \"Metodologia\": [\"Abordagem\", \"Justificativa\", \"Implementação\"],\n",
        "            \"Resultados\": [\"Precisão\", \"Entendimento\", \"Visualização\"],\n",
        "            \"Conclusão\": [\"Resumo\", \"Implicações\", \"Recomendações\"]\n",
        "        }\n",
        "        self.section_weights = {\n",
        "            \"Descrição do Problema\": 20,\n",
        "            \"Descrição dos Dados\": 30,\n",
        "            \"Metodologia\": 30,\n",
        "            \"Resultados\": 40,\n",
        "            \"Conclusão\": 30\n",
        "        }\n",
        "\n",
        "    def evaluate_section(self, report_section: str, criteria: list) -> float:\n",
        "        \"\"\"\n",
        "        Calcula a pontuação de uma seção do relatório com base em embeddings semânticos e critérios definidos.\n",
        "        \"\"\"\n",
        "        total_score = 0\n",
        "        max_score = 10 * len(criteria)  # Máximo de 10 pontos por critério\n",
        "\n",
        "        for criterion in criteria:\n",
        "            # Combina a seção com o critério e calcula a similaridade usando embeddings\n",
        "            embedding_section = self.model.encode(report_section, convert_to_tensor=True, truncation=True)\n",
        "            embedding_criterion = self.model.encode(criterion, convert_to_tensor=True, truncation=True)\n",
        "            similarity_score = util.pytorch_cos_sim(embedding_section, embedding_criterion).item() * 10\n",
        "\n",
        "            # Limita a pontuação máxima para cada critério em 10\n",
        "            total_score += min(10, similarity_score)\n",
        "\n",
        "        # Normaliza a pontuação da seção com base na quantidade de critérios\n",
        "        normalized_section_score = (total_score / max_score) * 10\n",
        "        return normalized_section_score\n",
        "\n",
        "    def evaluate_report(self, report: dict) -> float:\n",
        "        \"\"\"\n",
        "        Avalia o relatório inteiro, calculando a pontuação por seção com base na estrutura e critérios.\n",
        "        \"\"\"\n",
        "        total_score = 0\n",
        "        for section, criteria in self.section_prompts.items():\n",
        "            # Extrai o texto correspondente à seção no relatório e calcula a pontuação\n",
        "            section_text = report.get(section, \"\")\n",
        "            section_score = self.evaluate_section(section_text, criteria)\n",
        "            weighted_score = section_score * (self.section_weights[section] / 10)\n",
        "\n",
        "            total_score += weighted_score\n",
        "        return total_score  # Pontuação total ponderada do relatório\n",
        "\n",
        "\n",
        "# Classe de Análise de Código usando ferramentas externas de qualidade e segurança\n",
        "class CodeAnalyzer:\n",
        "    def analyze_code(self, code: str):\n",
        "        # Salva o código como um arquivo temporário para análise\n",
        "        with open(\"temp_code.py\", \"w\") as f:\n",
        "            f.write(code)\n",
        "\n",
        "        # Executa análise com Mypy, Ruff, Bandit e retorna contagem de erros\n",
        "        mypy_result = subprocess.run([\"mypy\", \"temp_code.py\"], capture_output=True, text=True)\n",
        "        ruff_result = subprocess.run([\"ruff\", \"temp_code.py\"], capture_output=True, text=True)\n",
        "        bandit_result = subprocess.run([\"bandit\", \"-r\", \"temp_code.py\"], capture_output=True, text=True)\n",
        "\n",
        "        errors_count = sum([mypy_result.returncode, ruff_result.returncode, bandit_result.returncode])\n",
        "        return errors_count, {\n",
        "            \"mypy\": mypy_result.stdout,\n",
        "            \"ruff\": ruff_result.stdout,\n",
        "            \"bandit\": bandit_result.stdout,\n",
        "        }"
      ],
      "metadata": {
        "id": "bWL74PkOmGtV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Treinamento do modelo"
      ],
      "metadata": {
        "id": "ielGBkudmQCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configura o logger para exibir no terminal e salvar logs para posterior análise\n",
        "custom_logger = configure(folder=\"logs/data_analysis_project\", format_strings=[\"stdout\", \"csv\", \"tensorboard\"])\n",
        "\n",
        "# Inicializa o ambiente e configura o PPO\n",
        "env = DataAnalysisEnv()\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1, device=device)\n",
        "model.set_logger(custom_logger)\n",
        "\n",
        "# Inicia o treinamento do modelo usando PPO\n",
        "model.learn(total_timesteps=10000)\n",
        "\n",
        "# Log detalhado de cada episódio após o término\n",
        "for episode in range(10):  # Número de episódios para log detalhado\n",
        "    env.log_episode(episode)"
      ],
      "metadata": {
        "id": "a9G5-uP7pGSL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68f74ff5-6563-4a7a-ce4e-0c7dbf2e353b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging to logs/data_analysis_project\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passo: Codificador ação: 1, Revisor ação: 1\n",
            "Código gerado pelo Codificador:\n",
            "\n",
            "        Agente Codificador:\n",
            "        Você é um programador no ramo da ciência de dados.\n",
            "        Seu trabalho é escrever um script para analisar os dados de vendas fornecidos.\n",
            "        O script deve tratar os dados e fazer uma análise temporal das vendas do arquivo \"/content/Walmart.csv\".\n",
            "        \n",
            "Faça uma limpeza nos dados. O programa vai gerar um arquivo temporário.\n",
            "            \"\"\"\n",
            "\n",
            "     \n",
            "    def caminho_arquivo():\n",
            "        Arquivo = os.getcwd() + '\\content\\Walmart.csv'\n",
            "        return Arquivo\n",
            "\n",
            "    def limpar_dados(caminho):\n",
            "        with open(caminho, 'w', encoding='utf-8', newline='') as arquivo:\n",
            "Revisão feita pelo Revisor:\n",
            "\n",
            "        Agente Revisor:\n",
            "        Você é um revisor de código no ramo da ciência de dados.\n",
            "        Seu trabalho é revisar o script de limpeza de dados e análise de dados fornecido pelo programador.\n",
            "        Você deve identificar erros e sugerir melhorias para garantir que o script seja eficiente e correto.\n",
            "        \n",
            "Execute o código para ver os resultados. Por exemplo:\n",
            "    python3 ScriptsModuul.py\n",
            "        \n",
            "\"\"\"\n",
            "\n",
            "import os   #Importo a biblioteca para limpar a tela\n",
            "\n",
            "dir = os.getcwd()  #Caminho do código\n",
            "ficha = []        #Arquivo para armazenar o conteúdo das linhas do arquivo\n",
            "\n",
            "def exibir(lista, c):  \n",
            "Recompensas - Codificador: -8, Revisor: 2.2196598491734933\n",
            "Recompensa total: -5.780340150826507\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passo: Codificador ação: 0, Revisor ação: 3\n",
            "Código gerado pelo Codificador:\n",
            "\n",
            "        Agente Codificador:\n",
            "        Você é um programador no ramo da ciência de dados.\n",
            "        Seu trabalho é escrever um script para analisar os dados de vendas fornecidos.\n",
            "        O script deve tratar os dados e fazer uma análise temporal das vendas do arquivo \"/content/Walmart.csv\".\n",
            "        \n",
            "Escreva um código de carregamento de dados.\n",
            "        \n",
            "        Deseja ver o arquivo?\n",
            "        Sim[s]: Sim\n",
            "        Não[n]: Não\n",
            "\n",
            "'''\n",
            "# Inicia o código.\n",
            "print(\"======================= Dados Automátizado =============================\")\n",
            "\n",
            "# Escreve o arquivo /content/Walmart.csv na mesma pasta /código.\n",
            "print(\"Seu arquivo é o: \\033[1\n",
            "Revisão feita pelo Revisor:\n",
            "\n",
            "        Agente Revisor:\n",
            "        Você é um revisor de código no ramo da ciência de dados.\n",
            "        Seu trabalho é revisar o script de limpeza de dados e análise de dados fornecido pelo programador.\n",
            "        Você deve identificar erros e sugerir melhorias para garantir que o script seja eficiente e correto.\n",
            "        \n",
            "Aprove o código se estiver correto.\n",
            "    \"\"\"\n",
            "Recompensas - Codificador: -8, Revisor: 0.0\n",
            "Recompensa total: -8.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "90LoX52amuVX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}