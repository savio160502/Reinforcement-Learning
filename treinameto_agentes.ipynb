{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!pip install pylint gymnasium stable_baselines3 shimmy"
      ],
      "metadata": {
        "id": "-WgFKeKngRXS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install shimmy"
      ],
      "metadata": {
        "id": "gKlpd4NVRYcg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install stable-baselines3 sentence-transformers mypy ruff bandit"
      ],
      "metadata": {
        "id": "p14Z1ipAWIA1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.logger import configure\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import subprocess\n",
        "import logging\n",
        "import torch\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "h4nP94_3lq0f"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar o uso da GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# device = \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "poZjNBoxNo9l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2d0e054-f253-4e44-d273-ba9ce9fd1076"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definindo o ambiente"
      ],
      "metadata": {
        "id": "RWdxnIX-l4Zs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataAnalysisEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(DataAnalysisEnv, self).__init__()\n",
        "\n",
        "        # Definindo o espaço de ações e observações para o ambiente\n",
        "        self.action_space = gym.spaces.Discrete(10)  # Ex.: 5 ações para codificador e 5 para revisor\n",
        "        self.observation_space = gym.spaces.Box(low=-1, high=1, shape=(512,), dtype=np.float32)\n",
        "\n",
        "        # Modelos LLM para Codificador e Revisor\n",
        "        self.codificador_llm = pipeline(\"text-generation\", model=\"Salesforce/codegen-350M-mono\")\n",
        "        self.revisor_llm = pipeline(\"text-generation\", model=\"Salesforce/codegen-350M-mono\")\n",
        "\n",
        "        # Variáveis de estado e controle de recompensas\n",
        "        self.state = None\n",
        "        self.done = False\n",
        "        self.rewards = {\"codificador\": 0, \"revisor\": 0}\n",
        "        self.total_score = 0\n",
        "        self.threshold_score = 135\n",
        "\n",
        "        # Prompt inicial para o agente codificador\n",
        "        self.prompt_codificador = \"\"\"\n",
        "        Agente Codificador:\n",
        "        Você é um programador no ramo da ciência de dados.\n",
        "        Seu trabalho é escrever um script para analisar os dados de vendas fornecidos.\n",
        "        O script deve tratar os dados e fazer uma análise temporal das vendas do arquivo \"Walmart.csv\".\n",
        "        \"\"\"\n",
        "\n",
        "        # Prompt inicial para o agente revisor\n",
        "        self.prompt_revisor = \"\"\"\n",
        "        Agente Revisor:\n",
        "        Você é um revisor de código no ramo da ciência de dados.\n",
        "        Seu trabalho é revisar o script de limpeza de dados e análise de dados fornecido pelo programador.\n",
        "        Você deve identificar erros e sugerir melhorias para garantir que o script seja eficiente e correto.\n",
        "        \"\"\"\n",
        "\n",
        "        # Avaliador de relatórios e log do episódio\n",
        "        self.report_evaluator = ReportEvaluator()\n",
        "        self.current_report = \"\"\n",
        "        self.logger = logging.getLogger(\"DataAnalysisEnv\")\n",
        "        self.logger.setLevel(logging.INFO)\n",
        "        self.episode_log = []\n",
        "        self.current_report_score = 0\n",
        "        self.previous_report_score = 0\n",
        "\n",
        "        self.analyzer = CodeAnalyzer() # analizar código\n",
        "\n",
        "    def reset(self):\n",
        "        # Redefine o ambiente e variáveis para um novo episódio\n",
        "        self.state = self._generate_initial_prompt()\n",
        "        self.done = False\n",
        "        self.total_score = 0\n",
        "        self.rewards = {\"codificador\": 0, \"revisor\": 0}\n",
        "        self.episode_log = []\n",
        "\n",
        "        # Reset de relatório e pontuação\n",
        "        self.current_report = \"\"\n",
        "        self.previous_report_score = 0\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        # Ações do Codificador e Revisor\n",
        "        codificador_action = action // 5\n",
        "        revisor_action = action % 5\n",
        "\n",
        "        # Armazena as ações no log do episódio\n",
        "        self.episode_log.append({\n",
        "            \"codificador_action\": codificador_action,\n",
        "            \"revisor_action\": revisor_action,\n",
        "            \"codificador_prompt\": self.prompt_codificador,\n",
        "            \"revisor_prompt\": self.prompt_revisor,\n",
        "        })\n",
        "\n",
        "        # Executa ações dos agentes\n",
        "        code = self._execute_codificador_action(codificador_action)\n",
        "        self._execute_revisor_action(revisor_action)\n",
        "\n",
        "        # Atualiza recompensas e verifica o término do episódio\n",
        "        self._update_rewards(code)\n",
        "        self.done = self.total_score >= self.threshold_score\n",
        "\n",
        "        return self.state, self.rewards, self.done, {}\n",
        "\n",
        "    def _execute_codificador_action(self, action):\n",
        "        # Mapeamento das ações do codificador usando a LLM\n",
        "        codificador_actions = [\n",
        "            \"Escreva um código de carregamento de dados.\",\n",
        "            \"Faça uma limpeza nos dados.\",\n",
        "            \"Realize uma análise temporal.\",\n",
        "            \"Visualize os resultados.\",\n",
        "            \"Interprete as conclusões.\"\n",
        "        ]\n",
        "        codificador_prompt = self.prompt_codificador + \"\\n\" + codificador_actions[action]\n",
        "        generated_code = self.codificador_llm(codificador_prompt, max_length=150)[0][\"generated_text\"]\n",
        "        self.current_report += generated_code\n",
        "\n",
        "        return generated_code\n",
        "\n",
        "    def _execute_revisor_action(self, action):\n",
        "        # Mapeamento das ações do revisor usando a LLM\n",
        "        revisor_actions = [\n",
        "            \"Verifique por erros no código.\",\n",
        "            \"Execute o código para ver os resultados.\",\n",
        "            \"Sugira melhorias de performance.\",\n",
        "            \"Aprove o código se estiver correto.\",\n",
        "            \"Melhore o relatório analítico.\"\n",
        "        ]\n",
        "        revisor_prompt = self.prompt_revisor + \"\\n\" + revisor_actions[action]\n",
        "        review_feedback = self.revisor_llm(revisor_prompt, max_length=100)[0][\"generated_text\"]\n",
        "        self.current_report += \"\\n\" + review_feedback\n",
        "\n",
        "    def _update_rewards(self,code):\n",
        "        # Calcula as recompensas para Codificador e Revisor\n",
        "        self.rewards[\"codificador\"] = self._calculate_codificador_reward(code)\n",
        "        self.rewards[\"revisor\"] = self._calculate_revisor_reward()\n",
        "        self.total_score += self.rewards[\"codificador\"] + self.rewards[\"revisor\"]\n",
        "\n",
        "    def _calculate_codificador_reward(self,code):\n",
        "        # Recompensa baseada na análise de código estático\n",
        "        errors_count, analysis_details = self.analyzer.analyze_code(code)\n",
        "\n",
        "        if errors_count > 0:\n",
        "            code_analysis_reward = -2 * errors_count  # Penalidade para cada erro\n",
        "        else:\n",
        "            code_analysis_reward = 10  # Recompensa para código sem erros\n",
        "\n",
        "        return code_analysis_reward\n",
        "\n",
        "    def _calculate_revisor_reward(self):\n",
        "        \"\"\"\n",
        "        Calcula a recompensa do revisor com base na melhoria na pontuação do relatório.\n",
        "        \"\"\"\n",
        "        # Gera um dicionário com as seções e textos do relatório atual\n",
        "        current_report_sections = {\n",
        "            \"Descrição do Problema\": self.current_report[:100],  # Exemplo de split\n",
        "            \"Descrição dos Dados\": self.current_report[100:200],\n",
        "            \"Metodologia\": self.current_report[200:300],\n",
        "            \"Resultados\": self.current_report[300:400],\n",
        "            \"Conclusão\": self.current_report[400:]\n",
        "        }\n",
        "\n",
        "        # Avalia o relatório atual e calcula a melhoria\n",
        "        current_report_score = self.report_evaluator.evaluate_report(current_report_sections)\n",
        "        self.total_score = max(0, current_report_score - self.previous_report_score)\n",
        "        self.previous_report_score = current_report_score  # Atualiza a pontuação anterior\n",
        "\n",
        "        # Normaliza a melhoria para um valor escalado de recompensa\n",
        "        if self.total_score >= self.threshold_score:\n",
        "          self.done = True\n",
        "\n",
        "        normalized_improvement = self.total_score / 150\n",
        "\n",
        "        return normalized_improvement * 10  # Multiplica para ajustar ao sistema de recompensas\n",
        "\n",
        "    def _generate_initial_prompt(self):\n",
        "        # Gera o estado inicial do prompt\n",
        "        return np.zeros(self.observation_space.shape)\n",
        "\n",
        "    def log_episode(self, episode_number):\n",
        "        # Gera um resumo detalhado do episódio\n",
        "        episode_summary = {\n",
        "            \"episode_number\": episode_number,\n",
        "            \"actions\": self.episode_log,\n",
        "            \"total_score\": self.total_score,\n",
        "            \"final_rewards\": self.rewards,\n",
        "        }\n",
        "        self.logger.info(f\"Resumo do Episódio {episode_number}: {episode_summary}\")\n"
      ],
      "metadata": {
        "id": "hIM6CbXXWBrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análise do relatório e do código gerado"
      ],
      "metadata": {
        "id": "4jjPChfumCzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReportEvaluator:\n",
        "    def __init__(self):\n",
        "        self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "        self.section_prompts = {\n",
        "            \"Descrição do Problema\": [\"Clareza\", \"Acurácia\"],\n",
        "            \"Descrição dos Dados\": [\"Completude\", \"Análise de Qualidade de Dados\", \"Visualização\"],\n",
        "            \"Metodologia\": [\"Abordagem\", \"Justificativa\", \"Implementação\"],\n",
        "            \"Resultados\": [\"Precisão\", \"Entendimento\", \"Visualização\"],\n",
        "            \"Conclusão\": [\"Resumo\", \"Implicações\", \"Recomendações\"]\n",
        "        }\n",
        "        self.section_weights = {\n",
        "            \"Descrição do Problema\": 20,\n",
        "            \"Descrição dos Dados\": 30,\n",
        "            \"Metodologia\": 30,\n",
        "            \"Resultados\": 40,\n",
        "            \"Conclusão\": 30\n",
        "        }\n",
        "\n",
        "    def evaluate_section(self, report_section: str, criteria: list) -> float:\n",
        "        \"\"\"\n",
        "        Calcula a pontuação de uma seção do relatório com base em embeddings semânticos e critérios definidos.\n",
        "        \"\"\"\n",
        "        total_score = 0\n",
        "        max_score = 10 * len(criteria)  # Máximo de 10 pontos por critério\n",
        "\n",
        "        for criterion in criteria:\n",
        "            # Combina a seção com o critério e calcula a similaridade usando embeddings\n",
        "            embedding_section = self.model.encode(report_section, convert_to_tensor=True)\n",
        "            embedding_criterion = self.model.encode(criterion, convert_to_tensor=True)\n",
        "            similarity_score = util.pytorch_cos_sim(embedding_section, embedding_criterion).item() * 10\n",
        "\n",
        "            # Limita a pontuação máxima para cada critério em 10\n",
        "            total_score += min(10, similarity_score)\n",
        "\n",
        "        # Normaliza a pontuação da seção com base na quantidade de critérios\n",
        "        normalized_section_score = (total_score / max_score) * 10\n",
        "        return normalized_section_score\n",
        "\n",
        "    def evaluate_report(self, report: dict) -> float:\n",
        "        \"\"\"\n",
        "        Avalia o relatório inteiro, calculando a pontuação por seção com base na estrutura e critérios.\n",
        "        \"\"\"\n",
        "        total_score = 0\n",
        "        for section, criteria in self.section_prompts.items():\n",
        "            # Extrai o texto correspondente à seção no relatório e calcula a pontuação\n",
        "            section_text = report.get(section, \"\")\n",
        "            section_score = self.evaluate_section(section_text, criteria)\n",
        "            weighted_score = section_score * (self.section_weights[section] / 10)\n",
        "\n",
        "            total_score += weighted_score\n",
        "        return total_score  # Pontuação total ponderada do relatório\n",
        "\n",
        "\n",
        "# Classe de Análise de Código usando ferramentas externas de qualidade e segurança\n",
        "class CodeAnalyzer:\n",
        "    def analyze_code(self, code: str):\n",
        "        # Salva o código como um arquivo temporário para análise\n",
        "        with open(\"temp_code.py\", \"w\") as f:\n",
        "            f.write(code)\n",
        "\n",
        "        # Executa análise com Mypy, Ruff, Bandit e retorna contagem de erros\n",
        "        mypy_result = subprocess.run([\"mypy\", \"temp_code.py\"], capture_output=True, text=True)\n",
        "        ruff_result = subprocess.run([\"ruff\", \"temp_code.py\"], capture_output=True, text=True)\n",
        "        bandit_result = subprocess.run([\"bandit\", \"-r\", \"temp_code.py\"], capture_output=True, text=True)\n",
        "\n",
        "        errors_count = sum([mypy_result.returncode, ruff_result.returncode, bandit_result.returncode])\n",
        "        return errors_count, {\n",
        "            \"mypy\": mypy_result.stdout,\n",
        "            \"ruff\": ruff_result.stdout,\n",
        "            \"bandit\": bandit_result.stdout,\n",
        "        }"
      ],
      "metadata": {
        "id": "bWL74PkOmGtV"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Treinamento do modelo"
      ],
      "metadata": {
        "id": "ielGBkudmQCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configura o logger para exibir no terminal e salvar logs para posterior análise\n",
        "custom_logger = configure(folder=\"logs/data_analysis_project\", format_strings=[\"stdout\", \"csv\", \"tensorboard\"])\n",
        "\n",
        "# Inicializa o ambiente e configura o PPO\n",
        "env = DataAnalysisEnv()\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1, ent_coef=0.01)\n",
        "model.set_logger(custom_logger)\n",
        "\n",
        "# Inicia o treinamento do modelo usando PPO\n",
        "model.learn(total_timesteps=10000)\n",
        "\n",
        "# Log detalhado de cada episódio após o término\n",
        "for episode in range(10):  # Número de episódios para log detalhado\n",
        "    env.log_episode(episode)"
      ],
      "metadata": {
        "id": "a9G5-uP7pGSL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}