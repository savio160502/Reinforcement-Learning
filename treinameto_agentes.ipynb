{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Definindo os agentes Codificador e Revisor"
      ],
      "metadata": {
        "id": "Y8DmdsopZQ9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Verifica se a GPU está disponível\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class LLMAgent:\n",
        "    def __init__(self, model_name=\"distilgpt2\", agent_type=\"codificador\"):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "        self.agent_type = agent_type\n",
        "        self.history = []\n",
        "        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
        "\n",
        "    def generate_code(self, prompt):\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        )\n",
        "\n",
        "        input_ids = inputs[\"input_ids\"].to(device)\n",
        "        attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "        outputs = self.model.generate(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=100,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=self.tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "        result = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        self.history.append({\"prompt\": prompt, \"result\": result})\n",
        "        return result\n",
        "\n",
        "    def review_code(self, code):\n",
        "        review_prompt = f\"Reveja o seguinte código e sugira melhorias:\\n\\n{code}\"\n",
        "        return self.generate_code(review_prompt)\n"
      ],
      "metadata": {
        "id": "BYOtb-TiZcJY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1MQ_4fjobQc",
        "outputId": "1a2872a6-4958-4373-b24d-3d56733af8f3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definindo o ambiente para os agentes interagirem"
      ],
      "metadata": {
        "id": "ccsOxfvZZkcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilizaremos a biblioteca Gym para definir um ambiente onde os agentes possam interagir.\n",
        "#  A função do ambiente é atribuir recompensas com base na qualidade do código e do feedback.\n",
        "\n",
        "import ast\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import random\n",
        "from timeout_decorator import timeout, TimeoutError\n",
        "import signal\n",
        "\n",
        "\n",
        "class DataAnalysisEnv(gym.Env):\n",
        "    def __init__(self, datasets, tasks, codificador, revisor, test_dataset=None):\n",
        "        super(DataAnalysisEnv, self).__init__()\n",
        "\n",
        "        self.codificador = codificador\n",
        "        self.revisor = revisor\n",
        "        self.datasets = datasets\n",
        "        self.test_dataset = test_dataset\n",
        "        self.tasks = tasks\n",
        "        self.reset_task_and_dataset()\n",
        "\n",
        "        self.current_state = 0\n",
        "        self.max_steps = 10\n",
        "        self.steps_taken = 0\n",
        "        self.done = False\n",
        "        self.current_code = \"\"\n",
        "\n",
        "        self.error_logged = set()  # Para registrar erros únicos\n",
        "        self.verbose_logging = True  # Alterar para True para mais detalhes\n",
        "\n",
        "        # Adicionando um atributo de histórico\n",
        "        self.history = []\n",
        "\n",
        "        # Definindo o espaço de observação e espaço de ação\n",
        "        self.observation_space = spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32)\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "\n",
        "    def reset_task_and_dataset(self):\n",
        "        self.current_task = random.choice(list(self.tasks.keys()))\n",
        "        self.dataset = random.choice(self.datasets)\n",
        "\n",
        "    def reset(self):\n",
        "        self.reset_task_and_dataset()\n",
        "        self.current_state = 0\n",
        "        self.steps_taken = 0\n",
        "        self.done = False\n",
        "        self.current_code = \"\"\n",
        "        return np.array([self.current_state], dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        self.steps_taken += 1\n",
        "        reward = 0\n",
        "\n",
        "        if action == 0:\n",
        "            new_code = self.codificador.generate_code(f\"Dados: {self.dataset}\\nTarefa: {self.tasks[self.current_task]['description']}\")\n",
        "            self.current_code += new_code\n",
        "            reward = -1\n",
        "\n",
        "        elif action == 1:\n",
        "            feedback = self.revisor.review_code(self.current_code)\n",
        "            reward = 1\n",
        "\n",
        "        elif action == 2:\n",
        "            if self._evaluate_solution():\n",
        "                reward = 10\n",
        "            else:\n",
        "                reward = -10\n",
        "            self.done = True\n",
        "\n",
        "        self.current_state = self.steps_taken / self.max_steps\n",
        "        if self.steps_taken >= self.max_steps:\n",
        "            self.done = True\n",
        "\n",
        "        # Atualizando o histórico\n",
        "        self.history.append({'reward': reward, 'steps': self.steps_taken, 'state': self.current_state})\n",
        "\n",
        "        if self.verbose_logging:\n",
        "            print(f\"Ação executada: {action}, Estado atual: {self.current_state:.2f}, Recompensa: {reward}, Done: {self.done}\")\n",
        "\n",
        "        return np.array([self.current_state], dtype=np.float32), reward, self.done, {}\n",
        "\n",
        "    def _evaluate_solution(self):\n",
        "        task_info = self.tasks[self.current_task]\n",
        "        required_components = task_info[\"required_components\"]\n",
        "\n",
        "        try:\n",
        "            tree = ast.parse(self.current_code)\n",
        "        except SyntaxError as e:\n",
        "            error_message = f\"Erro de sintaxe: {e}\"\n",
        "            if error_message not in self.error_logged:\n",
        "                self.error_logged.add(error_message)\n",
        "            return False\n",
        "\n",
        "        components_found = set()\n",
        "        for node in ast.walk(tree):\n",
        "            if isinstance(node, ast.Call) and isinstance(node.func, ast.Attribute):\n",
        "                if node.func.attr == 'plot' and 'line_chart' in required_components:\n",
        "                    components_found.add('line_chart')\n",
        "                if node.func.attr == 'scatter' and 'scatter_plot' in required_components:\n",
        "                    components_found.add('scatter_plot')\n",
        "                if node.func.attr == 'hist' and 'histogram' in required_components:\n",
        "                    components_found.add('histogram')\n",
        "\n",
        "        if not components_found:\n",
        "            message = \"Nenhuma função de visualização necessária foi encontrada no código.\"\n",
        "            if message not in self.error_logged:\n",
        "                self.error_logged.add(message)\n",
        "            return False\n",
        "\n",
        "        # Configura o timeout\n",
        "        signal.signal(signal.SIGALRM, self.handler)\n",
        "        signal.alarm(5)  # Define o tempo limite para 5 segundos\n",
        "\n",
        "        try:\n",
        "            buf = io.BytesIO()\n",
        "            plt.switch_backend('Agg')\n",
        "            exec(self.current_code, {\"plt\": plt, \"io\": io, \"buf\": buf})\n",
        "\n",
        "            if buf.tell() == 0:\n",
        "                if \"Nenhum gráfico foi gerado.\" not in self.error_logged:\n",
        "                    self.error_logged.add(\"Nenhum gráfico foi gerado.\")\n",
        "                return False\n",
        "\n",
        "            buf.close()\n",
        "\n",
        "        except TimeoutError:\n",
        "            print(\"Execução do código excedeu o tempo limite.\")\n",
        "            return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao executar o código: {e}\")\n",
        "            return False\n",
        "        finally:\n",
        "            signal.alarm(0)  # Cancela o alarme\n",
        "\n",
        "        print(\"Código gerado corretamente e pelo menos um gráfico foi plotado.\")\n",
        "        return True\n",
        "\n",
        "    def handler(self, signum, frame):\n",
        "        \"\"\"Função chamada quando o tempo limite é atingido.\"\"\"\n",
        "        raise TimeoutError\n",
        "\n",
        "# Codificador e Revisor no Ambiente: O ambiente recebe o codificador e o revisor como parâmetros e usa suas funções para gerar e revisar o código em cada passo.\n",
        "\n",
        "# Ação do Codificador: Quando o agente decide continuar gerando código (action == 0), a função generate_code do Codificador é chamada e o código é atualizado no ambiente.\n",
        "\n",
        "# Ação do Revisor: Quando o agente decide revisar o código (action == 1), o Revisor revisa o código gerado e sugere melhorias.\n",
        "\n",
        "# Finalizar: Se o agente decide finalizar o processo (action == 2), a função de avaliação compara o código atual com a solução esperada e atribui uma recompensa dependendo do resultado."
      ],
      "metadata": {
        "id": "UNUWfoO5Zv6E"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Treinamento dos agentes"
      ],
      "metadata": {
        "id": "72qqUJcDZ3Rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Usaremos a biblioteca stable-baselines3 para realizar o treinamento dos agentes.\n",
        "# Usaremos o algoritmo PPO (Proximal Policy Optimization), que é adequado para esse tipo de problema\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "import torch\n",
        "\n",
        "def train_agents():\n",
        "    \"\"\"\n",
        "    Treina os agentes codificador e revisor no ambiente de análise de dados.\n",
        "    \"\"\"\n",
        "    # Instanciando os agentes LLM\n",
        "    codificador = LLMAgent(agent_type=\"codificador\")\n",
        "    revisor = LLMAgent(agent_type=\"revisor\")\n",
        "\n",
        "    # Instanciando o ambiente de RL\n",
        "    env = DataAnalysisEnv(datasets, tasks, codificador, revisor)\n",
        "\n",
        "    # Configurando o modelo de RL PPO para usar GPU, se disponível\n",
        "    model = PPO(\"MlpPolicy\", env, verbose=1, device=device)\n",
        "\n",
        "    # Treinamento do agente\n",
        "    model.learn(total_timesteps=10000)\n",
        "\n",
        "    # Visualizando o progresso durante o treinamento\n",
        "    visualize_progress(env.history)\n",
        "\n",
        "    return env, model\n",
        "\n",
        "def test_agent(env, model):\n",
        "    \"\"\"\n",
        "    Testa o agente treinado em um novo episódio e exibe seu desempenho.\n",
        "    \"\"\"\n",
        "    env.datasets = [env.test_dataset]  # Usar apenas o dataset de teste\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    env.history = []  # Resetando o histórico para o teste\n",
        "\n",
        "    while not done:\n",
        "        action, _states = model.predict(obs)\n",
        "        obs, reward, done, info = env.step(action)\n",
        "\n",
        "        # Salvando a interação para visualização posterior\n",
        "        env.history.append({\n",
        "            'state': obs,\n",
        "            'reward': reward,\n",
        "            'action': action\n",
        "        })\n",
        "\n",
        "        print(f\"Recompensa: {reward}, Estado: {obs}\")\n",
        "\n",
        "    # Visualiza o progresso após o teste\n",
        "    visualize_progress(env.history)\n",
        "\n",
        "def visualize_progress(history):\n",
        "    \"\"\"\n",
        "    Exibe o progresso do agente ao longo do tempo, com base nas interações e recompensas recebidas.\n",
        "    \"\"\"\n",
        "    rewards = [entry['reward'] for entry in history]\n",
        "    plt.plot(rewards)\n",
        "    plt.title(\"Progresso do Agente\")\n",
        "    plt.xlabel(\"Passos\")\n",
        "    plt.ylabel(\"Recompensa\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "Sxx7DUq8Z8Y9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install gymnasium==0.29\n",
        "#!pip install shimmy==1.3.0"
      ],
      "metadata": {
        "id": "CRtoYgx2q0SV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Datasets e Tarefas\n",
        "datasets = [\"/content/Walmart.csv\", \"/content/bestsellers with categories.csv\",\n",
        "            \"/content/bestsellers_categories.csv\",\"/content/sell_house.csv\",\n",
        "            \"/content/sell_phones.csv\"]  # Datasets de treino\n",
        "\n",
        "test_dataset = \"/content/laptop_price.csv\"  # Dataset de teste\n",
        "\n",
        "tasks = {\n",
        "    \"visualization\": {\n",
        "        \"description\": \"Gerar gráficos com dados.\",\n",
        "        \"required_components\": [\"line_chart\", \"scatter_plot\", \"histogram\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Treina os agentes\n",
        "env, model = train_agents()\n",
        "\n",
        "# Atualiza o ambiente com o dataset de teste\n",
        "env.test_dataset = test_dataset\n",
        "\n",
        "# Testa o agente treinado\n",
        "test_agent(env, model)"
      ],
      "metadata": {
        "id": "enpJm9XwR1D2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40cedfb4-b589-4f8d-d0c7-521bd69fc1ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "e9H6DpUu_KMj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}